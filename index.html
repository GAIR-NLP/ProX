<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="ProX: Lifting Pre-training Data Quality Like Experts">
    <meta property="og:image" content="./static/icons/banner.jpg" />
    <meta name="keywords" content="LLM Pre-training, Data Refining, Programming">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ProX: Lifting Pre-training Data Quality Like Experts</title>

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
    <script>
        window.dataLayer = window.dataLayer || [];

        function gtag() {
            dataLayer.push(arguments);
        }

        gtag('js', new Date());

        gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/icons/banner.jpg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>




<nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
    </div>

    </div>
</nav>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title"><span style="color: hsl(204, 86%, 53%);">Pro</span>gramming Every E<span style="color: hsl(204, 86%, 53%);">x</span>ample: Lifting Pre-training Data Quality Like Experts at Scale</em></h1>
                    <div class="is-size-5 publication-authors">
                    <span class="author-block">
                    <a href="https://koalazf99.github.io/">Fan Zhou</a><sup>1,4,*</sup>,
                    </span>
                    <span class="author-block">
                    <a href="http://tinyurl.com/zengzhi-homepage/">Zengzhi Wang</a><sup>1,4,*</sup>,
                    </span>
                    <span class="author-block">
                    <a href="https://siviltaram.github.io/">Qian Liu</a><sup>3</sup>,
                    </span>
                    <span class="author-block">
                    <a href="https://lockon-n.github.io/">Junlong Li</a><sup>4</sup>,
                    </span>
                    <span class="author-block">
                    <a href="https://plms.ai/people/index.html/">Pengfei Liu</a><sup>1,2,4,+</sup>,
                    </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University</span>

                        <span class="author-block"><sup>2</sup>Shanghai Artificial Intelligence Laboratory</span>
                        <br>
                        <span class="author-block"><sup>3</sup>Sea AI Lab</span>
                        <span class="author-block"><sup>4</sup>Generative Artificial Intelligence Research Lab (GAIR)</span>
                        <br>
                        <span class="author-block"><sup>*</sup>Equal Contribution</span>
                        <span class="author-block"><sup>+</sup>Corresponding Author</span>
                    </div>

                    <div class="is-size-5 publication-authors" style="margin-bottom: 1rem;">
                        <span class="author-block">
                          <div style="display: flex; align-items: center;">
                            <span style="vertical-align: middle;">
                                <a href="https://en.sjtu.edu.cn/"><img src="./static/icons/sjtu-logo.svg" style="height: 5.2rem; margin-right: 5px; vertical-align: middle;"></a>
                                <a href="https://www.shlab.org.cn/"><img src="./static/icons/shai-logo.png" style="height: 4.3rem; margin-right: 5px; vertical-align: middle;"></a>
                                <a href="https://sail.sea.com/"><img src="./static/icons/seaai-logo.png" style="height: 4.5rem; margin-right: 5px; vertical-align: middle;"></a>
                                <a href="https://plms.ai/people/index.html"><img src="./static/icons/gair-logo.png" style="height: 5rem; margin-right: 5px; vertical-align: middle;"></a>
                            </span>
                        </div>
                        </span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">

                        
                        <!-- PDF Link. -->
                        <span class="link-block">
                            <a href="https://arxiv.org/abs/xxxx.xxxxx" class="external-link button is-normal is-rounded is-dark">
                            <span class="icon">
                                                <!-- <svg class="svg-inline--fa fa-file-pdf fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file-pdf" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M181.9 256.1c-5-16-4.9-46.9-2-46.9 8.4 0 7.6 36.9 2 46.9zm-1.7 47.2c-7.7 20.2-17.3 43.3-28.4 62.7 18.3-7 39-17.2 62.9-21.9-12.7-9.6-24.9-23.4-34.5-40.8zM86.1 428.1c0 .8 13.2-5.4 34.9-40.2-6.7 6.3-29.1 24.5-34.9 40.2zM248 160h136v328c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V24C0 10.7 10.7 0 24 0h200v136c0 13.2 10.8 24 24 24zm-8 171.8c-20-12.2-33.3-29-42.7-53.8 4.5-18.5 11.6-46.6 6.2-64.2-4.7-29.4-42.4-26.5-47.8-6.8-5 18.3-.4 44.1 8.1 77-11.6 27.6-28.7 64.6-40.8 85.8-.1 0-.1.1-.2.1-27.1 13.9-73.6 44.5-54.5 68 5.6 6.9 16 10 21.5 10 17.9 0 35.7-18 61.1-61.8 25.8-8.5 54.1-19.1 79-23.2 21.7 11.8 47.1 19.5 64 19.5 29.2 0 31.2-32 19.7-43.4-13.9-13.6-54.3-9.7-73.6-7.2zM377 105L279 7c-4.5-4.5-10.6-7-17-7h-6v128h128v-6.1c0-6.3-2.5-12.4-7-16.9zm-74.1 255.3c4.1-2.7-2.5-11.9-42.8-9 37.1 15.8 42.8 9 42.8 9z"></path></svg> -->
                                                <!-- <i class="fas fa-file-pdf"></i> Font Awesome fontawesome.com -->
                                                <i class="ai ai-arxiv"></i>
                            </span>
                            <span>Paper</span>
                            </a>
                        </span>
                        <!-- Code Link. -->
                        <span class="link-block">
                            <a href="https://github.com/GAIR-NLP/program-every-example" class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                            <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                                        </span>
                                <span>Code</span>
                            </a>
                        </span>
                        <!-- Data Link. -->
                        <!-- <span class="link-block">
                            <a href="https://huggingface.co/collections/GAIR/xxxx"
                               class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <img src="./static/icons/huggingface_logo.svg" width="20px">
                                </span>
                               <span>Data</span>
                               </a>
                        </span> -->
                        <!-- Model Link. -->
                        <span class="link-block">
                            <a href="https://huggingface.co/gair-prox"
                               class="external-link button is-normal is-rounded is-dark">
                                <span class="icon">
                                    <img src="./static/icons/huggingface_logo.svg" width="20px">
                                </span>
                               <span>HF Repo</span>
                               </a>
                        </span>
                        <span class="link-block">
                            <a href="https://x.com/xx" target="_blank"
                               class="external-link button is-normal is-rounded is-dark">
                              <span class="icon">
                                  <i class="fab fa-twitter"></i>
                              </span>
                              <span>Official Twitter</span>
                            </a>
                        </span>
                        <span class="link-block">
                            <a href="https://x.com/xx" target="_blank"
                               class="external-link button is-normal is-rounded is-dark">
                              <span class="icon">
                                  <i class="fab fa-twitter"></i>
                              </span>
                              <span>Twitter By AK</span>
                            </a>
                        </span>
                    </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>


<section class="section teaser">
    <div class="container is-max-desktop">
  <!-- Abstract. -->
  <div class="columns is-centered has-text-centered" style="font-size: 16px;">
  <div class="column is-four-fifths">
      <h1 class="title is-3">Introduction Video for ProX</h1>
    <iframe 
    src="static/images/prox-intro-gif.mp4" 
    width="740" 
    height="500" 
    allow="autoplay" 
    allowfullscreen>
    </iframe>
</div>
</div>
</div>
</section>



<section class="section teaser">
  <div class="container is-max-desktop">
<!-- Abstract. -->
<div class="columns is-centered has-text-centered" style="font-size: 16px;">
<div class="column is-four-fifths">
    <h1 class="title is-3">Overview</h1>
    <div class="content has-text-justified" >

        <div>
        <p>
            Still relies heavily on human experts crafted rules to improve the quality of pre-training corpora? Depsite its efficency, these rules lack the flexibility to address the unique
            characteristics of individual example effectively and struggle to handle fine-gained cleaning operations. Meanwhile, applying tailored rules to every example is impractical for human experts.
        </p> 
        <p>üöÄ We introduce <i>Programming Every Example</i> (<span style="color: hsl(204, 86%, 53%);"><b>ProX</b></span>), a novel framework that treats data refinement as a programming task, enabling models to refine corpora by generating and executing fine-grained operations, such as string normalization, for each individual example at scale. 
        ProX go through 2 refining stages focusing on different scopes: <b>document level</b> and <b>chunk level</b>. We use very small language models (e.g., üê£0.3B) to generate programs for each example in the training data, and then use these programs together with a Python Executor to refine the data. </p>
        <p>
        We demonstrate that ProX could significantly improve the quality of pre-training corpora.
        <ul>  
            <li>A üêò1.7B language model trained on corpus refined by ProX with 50B tokens training perform similarly to <a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T">TinyLlama-1.1B-3T</a>, which saves 30 times training compute!</li>
            <li>ProX works well across various corpora (e.g., <a href="https://huggingface.co/datasets/allenai/c4">C4</a>, <a href="https://huggingface.co/datasets/togethercomputer/RedPajama-Data-V2">RedPajama-v2</a>, <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb">FineWeb</a>) and model sizes</li>
            <li>ProX can refine domain-specific corpora (such as math) without customization, boosting math reasoning performance by 7%~20% for models like <a href=https://huggingface.co/mistralai/Mistral-7B-v0.1>Mistral-7B</a> and <a href=https://huggingface.co/meta-llama/Llama-2-7b-hf>Llama-7B</a>. Additionally, ProX builds a performance-comparable <a href="https://huggingface.co/EleutherAI/llemma_7b">Llemma-7B</a> model with 20x less training compute</li>
            <li>Quantitative analysis demonstreate that investment compute on the inference side to refine corpora is totally worthwhile.</li>
        </ul>
        </p>
        </div>
    </div>
</div>
</div>


<br>

<div class="columns is-centered " style="font-size: 16px;">
    <div class="column is-four-fifths">
        <h1 class="title is-3">Programming Every Example</h1>
        <div class="content has-text-justified">
            <p>To keep things lean, the steps are generally as follows: </p>
            <p><b>Step 1:</b> Adapt a base language model (typically less than 1B) to perform data refinement via fine-tuning on synthetic supervised data. (How to adapt? see belowüëá)</p> 
            <p><b>Step 2:</b> Apply this refinement model to generate programs for each docment in the corpus</p>
            <p><b>Step 3:</b> Execute the generated programs for each docment, producing refined corpus ready for pre-training.</p>
            <br>
            <div class="content has-text-justified">
                <figure>
                    <img src="./static/images/prox_framework.png" style="zoom: 60%;" alt="Description of first image">
                    <figcaption>
                        ProX Frameworks, lifting pre-training data quality by generating programs for each example in the training data.
                    </figcaption>
                </figure>
            </div>
        </div>
    </div>
</div>


<br>


<div class="columns is-centered" style = "font-size: 16px;">
<div class="column is-four-fifths">
    <h1 class="title is-3">How to adpat a LM to perform data refinement</h1>
    <div class="content has-text-justified">
        <figure>
            <img src="./static/images/prox_pipeline.png" style="zoom: 60%;" alt="Description of first image">
            <figcaption>
                ProX Adaptation Pipeline.
            </figcaption>
        </figure>
        <p>To keep things lean, the steps are generally as follows: </p>
        <p><b>Step 1:</b> Prompt a advanced language model (i.e., Llama-3 series) to annotate random seed documents based on designed prompts (including scoring criteria, function interface and description)</p>
        <p><b>Step 2:</b> Supervisedly fine-tine a base language model (often less than 1B) on the synthetic data</p>
        <p><i>Then, we can employ this refinement model to annotate docments from the corpus at scale.</i> </p>

        <!-- Second figure -->
        <!-- <figure>
            <img src="./static/images/program_space.png" style="zoom: 40%;" alt="Description of second image">
            <figcaption>Prox Program Design.
            </figcaption>
        </figure> -->
        <table border="1" cellspacing="1" cellpadding="10">
            <caption>ProX Program Design</caption>
            <tr>
              <th>Stage</th>
              <th>Function Interface</th>
              <th>Function Description</th>
            </tr>
            <tr>
              <td rowspan="2">Document<br>Level</td>
              <td>drop_doc()</td>
              <td>Delete the document from corpus.</td>
            </tr>
            <tr>
              <td>keep_doc()</td>
              <td>Keep the document in corpus.</td>
            </tr>
            <tr>
              <td rowspan="3">Chunk<br>Level</td>
              <td>remove_lines(line_start:int, line_end:int)</td>
              <td>Delete noisy lines from the chunk.</td>
            </tr>
            <tr>
              <td>normalize(source_str:str, target_str:str)</td>
              <td>Replace strings with normalized ones.</td>
            </tr>
            <tr>
              <td>skip_chunk()</td>
              <td>Keep the chunk as it is.</td>
            </tr>
          </table>  
    </div>
</div>
</div>
<!-- Demo -->


<br>


<div class="columns is-centered">
    <!-- Performance-->
    <div class="column is-full-width">
      <h2 class="title is-2">Benchmarking Performance</h2>
     
      <div class="content has-text-justified">
        <p>
          <b>TL;DR</b>: ProX could significantly lift the quality of the pre-training corpora (e.g., C4, RedPajama-V2, FineWeb and OpenWebMath), enabling the creation of performance-equivalent language models with far fewer training FLOPs.
        </p>

        <div class="content has-text-justified">
            <figure>
                <img src="./static/images/prox-on-fineweb-owm.png" style="zoom: 60%;" alt="Description of the image">
                <figcaption>
                    Training FLOPs v.s. average downstream performance. Although these corpora have gone through expert-crafted rules, applying PROX still yields significant improvements over these baseline models trained with original data corpus. Moreover, with much less training FLOPs, model trained on PROX curated data show comparable performance with existing models.
                </figcaption>
            </figure>
        </div>
        
      </div>
      <!-- Benchmark. -->
      <br/>

      <div class="column is-full-width">
        <h2 class="title is-4">Main Results: applying on RedPajama-V2</h2>

        <p>
          We first conduct a series of experiments to verify the effectiveness of each ProX operation. We begin by training a 750M on the RedPajama-V2 raw data for approximately 26B tokens (or 12.5K steps) as the initial baseline. Then, we apply different data refinement approaches (i.e., various rules and ProX) to the raw data and pre-train the model from scratch with the same steps. 
          We evaluate the performance across ten selected tasks using lighteval's implementation (Fourrier et al., 2023), and report the zero-shot accuracy. </p>
        <br/>
        
        <style type="text/css">
            .tg  {border-collapse:collapse;border-spacing:0;}
            .tg td{border-color:#ffffff;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
              overflow:hidden;padding:10px 5px;word-break:normal;}
            .tg th{border-color:#ffffff;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
              font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
            .tg .tg-c3ow{border-color:#ffffff;text-align:center;vertical-align:top} 
            .tg .tg-fymr{border-color:#ffffff;font-weight:bold;text-align:left;vertical-align:top}
            .tg .tg-7btt{border-color:#ffffff;font-weight:bold;text-align:center;vertical-align:top}
            .tg .tg-0pky{border-color:#ffffff;text-align:left;vertical-align:top}
            </style>
            <table class="tg"><thead>
              <tr>
                <th class="tg-fymr">Method</th>
                <th class="tg-7btt">ARC-C</th>
                <th class="tg-7btt">ARC-E</th>
                <th class="tg-7btt">CSQA</th>
                <th class="tg-7btt">HellaS</th>
                <th class="tg-7btt">MMLU</th>
                <th class="tg-7btt">OBQA</th>
                <th class="tg-7btt">PIQA</th>
                <th class="tg-7btt">SIQA</th>
                <th class="tg-7btt">WinoG</th>
                <th class="tg-7btt">SciQ</th>
                <th class="tg-7btt">AVG</th>
                <th class="tg-7btt"># Win</th>
              </tr></thead>
            <tbody>
              <tr>
                <td class="tg-0pky">Raw</td>
                <td class="tg-c3ow">26.1</td>
                <td class="tg-c3ow">44.3</td>
                <td class="tg-c3ow">29.7</td>
                <td class="tg-c3ow">39.1</td>
                <td class="tg-c3ow">27.3</td>
                <td class="tg-c3ow">29.2</td>
                <td class="tg-c3ow">66.9</td>
                <td class="tg-c3ow">39.0</td>
                <td class="tg-c3ow">52.0</td>
                <td class="tg-c3ow">67.4</td>
                <td class="tg-0pky">42.1</td>
                <td class="tg-0pky">1 / 10</td>
              </tr>
              <tr>
                <td class="tg-c3ow" colspan="13">Apply Existing Rules</td>
              </tr>
              <tr>
                <td class="tg-0pky">Gopher Rules</td>
                <td class="tg-c3ow">25.7</td>
                <td class="tg-c3ow">44.0</td>
                <td class="tg-c3ow">31.3</td>
                <td class="tg-c3ow">40.2</td>
                <td class="tg-c3ow">27.3</td>
                <td class="tg-c3ow">29.0</td>
                <td class="tg-c3ow">66.3</td>
                <td class="tg-c3ow">39.0</td>
                <td class="tg-c3ow">51.2</td>
                <td class="tg-c3ow">68.9</td>
                <td class="tg-0pky">42.3</td>
                <td class="tg-0pky">0 / 10</td>
              </tr>
              <tr>
                <td class="tg-0pky">C4 Rules</td>
                <td class="tg-c3ow">25.0</td>
                <td class="tg-c3ow">46.0</td>
                <td class="tg-c3ow">31.0</td>
                <td class="tg-c3ow">40.5</td>
                <td class="tg-c3ow">27.1</td>
                <td class="tg-c3ow">29.2</td>
                <td class="tg-7btt">68.5</td>
                <td class="tg-7btt">40.5</td>
                <td class="tg-c3ow">51.7</td>
                <td class="tg-c3ow">66.6</td>
                <td class="tg-0pky">42.6</td>
                <td class="tg-0pky">2 / 10</td>
              </tr>
              <tr>
                <td class="tg-0pky">FineWeb Rules</td>
                <td class="tg-c3ow">25.2</td>
                <td class="tg-c3ow">46.8</td>
                <td class="tg-7btt">32.6</td>
                <td class="tg-c3ow">39.6</td>
                <td class="tg-c3ow">27.2</td>
                <td class="tg-c3ow">29.0</td>
                <td class="tg-c3ow">66.5</td>
                <td class="tg-c3ow">39.4</td>
                <td class="tg-7btt">52.4</td>
                <td class="tg-c3ow">69.2</td>
                <td class="tg-0pky">42.8</td>
                <td class="tg-0pky">2 / 10</td>
              </tr>
              <tr>
                <td class="tg-0pky">Gopher + C4 + FineWeb</td>
                <td class="tg-c3ow">25.2</td>
                <td class="tg-c3ow">43.9</td>
                <td class="tg-c3ow">30.0</td>
                <td class="tg-c3ow">41.9</td>
                <td class="tg-c3ow">27.5</td>
                <td class="tg-c3ow">31.0</td>
                <td class="tg-c3ow">67.0</td>
                <td class="tg-c3ow">39.9</td>
                <td class="tg-c3ow">51.9</td>
                <td class="tg-c3ow">65.3</td>
                <td class="tg-0pky">42.3</td>
                <td class="tg-0pky">0 / 10</td>
              </tr>
              <tr>
                <td class="tg-c3ow" colspan="13">ProX(Ours): D = Document-level programming,  C = Chunk-level programming.</td>
              </tr>
              <tr>
                <td class="tg-0pky"><span style="color: hsl(204, 86%, 53%);">ProX-D </span></td>
                <td class="tg-7btt">26.6</td>
                <td class="tg-c3ow">49.7</td>
                <td class="tg-c3ow">30.1</td>
                <td class="tg-c3ow">40.5</td>
                <td class="tg-7btt">29.4</td>
                <td class="tg-c3ow">30.4</td>
                <td class="tg-c3ow">66.3</td>
                <td class="tg-c3ow">39.0</td>
                <td class="tg-c3ow">51.2</td>
                <td class="tg-c3ow">71.6</td>
                <td class="tg-0pky">43.5</td>
                <td class="tg-0pky">2 / 10</td>
              </tr>
              <tr>
                <td class="tg-0pky"><span style="color: hsl(204, 86%, 53%);">ProX-D+C</span></td>
                <td class="tg-c3ow">26.4</td>
                <td class="tg-7btt">51.9</td>
                <td class="tg-c3ow">30.9</td>
                <td class="tg-7btt">42.4</td>
                <td class="tg-7btt">29.4</td>
                <td class="tg-7btt">31.6</td>
                <td class="tg-c3ow">67.9</td>
                <td class="tg-c3ow">40.0</td>
                <td class="tg-c3ow">52.2</td>
                <td class="tg-7btt">73.5</td>
                <td class="tg-fymr">44.6</td>
                <td class="tg-fymr">5 / 10</td>
              </tr>
            </tbody></table>
        

      </div>

      <br>

      <div class="column is-full-width">
        <h2 class="title is-5">Beyond heuristic rules, ProX also outperforms existing data selection methods</h2>
        
        <p>
        We adopt the setting of <a href="https://arxiv.org/abs/2406.06046">MATES (Yu et al., 2024)</a> to enable a controlled comparison with existing data selection methods, showcasing the effectiveness of ProX. Specifically, we apply ProX to the C4 dataset and pre-train the model using the Pythia architecture. We report both zero-shot and 2-shot performance using the LM-eval-harness.
        </p>
        <br/>

        <style type="text/css">
            /* .tg  {border-collapse:collapse;border-spacing:0;} */
            /* .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
              overflow:hidden;padding:10px 5px;word-break:normal;} */
            /* .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
              font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} */
            .tg .tg-km2t{border-color:#ffffff;font-weight:bold;text-align:left;vertical-align:top}
            .tg .tg-zv4m{border-color:#ffffff;text-align:left;vertical-align:top}
            .tg .tg-8jgo{border-color:#ffffff;text-align:center;vertical-align:top}
            .tg .tg-aw21{border-color:#ffffff;font-weight:bold;text-align:center;vertical-align:top}
            </style>
            <table class="tg"><thead>
              <tr>
                <th class="tg-km2t">Method</th>
                <th class="tg-aw21">0-shot</th>
                <th class="tg-aw21">2-shot</th>
                <th class="tg-aw21"># Win</th>
              </tr></thead>
            <tbody>
              <tr>
                <td class="tg-8jgo" colspan="4"><b>Model Architecture: Pythia-410M</b></td>
              </tr>
              <tr>
                <td class="tg-zv4m">Random</td>
                <td class="tg-8jgo">42.7</td>
                <td class="tg-8jgo">43.8</td>
                <td class="tg-8jgo">0 / 8</td>
              </tr>
              <tr>
                <td class="tg-zv4m"><a href="https://openreview.net/forum?id=uPSQv0leAu">DSIR (Xie et al., 2023)</a></td>
                <td class="tg-8jgo">42.5</td>
                <td class="tg-8jgo">43.7</td>
                <td class="tg-8jgo">1 / 8</td>
              </tr>
              <tr>
                <td class="tg-zv4m"><a href="https://arxiv.org/abs/2401.12926">DsDM (Engstom et al., 2024)</a></td>
                <td class="tg-8jgo">43.4</td>
                <td class="tg-8jgo">44.1</td>
                <td class="tg-8jgo">0 / 8</td>
              </tr>
              <tr>
                <td class="tg-zv4m"><a href="https://proceedings.mlr.press/v235/wettig24a.html">QuRating (Wettig et al., 2024)</a></td>
                <td class="tg-8jgo">43.5</td>
                <td class="tg-8jgo">44.6</td>
                <td class="tg-8jgo">0 / 8</td>
              </tr>
              <tr>
                <td class="tg-zv4m"><a href="https://arxiv.org/abs/2406.06046">MATES (Yu et al., 2024)</a></td>
                <td class="tg-8jgo">44.0</td>
                <td class="tg-8jgo">45.0</td>
                <td class="tg-8jgo">0 / 8</td>
              </tr>
              <tr>
                <td class="tg-zv4m"><span style="color: hsl(204, 86%, 53%);">ProX</span></td>
                <td class="tg-aw21">46.2</td>
                <td class="tg-aw21">47.5</td>
                <td class="tg-aw21">7 / 8</td>
              </tr>
              <tr>
                <td class="tg-8jgo" colspan="4"><b>Model Architecture: Pythia-1B</b></td>
              </tr>
              <tr>
                <td class="tg-zv4m">Random</td>
                <td class="tg-8jgo">44.7</td>
                <td class="tg-8jgo">45.4</td>
                <td class="tg-8jgo">0 / 8</td>
              </tr>
              <tr>
                <td class="tg-zv4m"><a href="https://arxiv.org/abs/2406.06046">MATES (Yu et al., 2024)</a></td>
                <td class="tg-8jgo">45.8</td>
                <td class="tg-8jgo">46.4</td>
                <td class="tg-8jgo">1 / 8</td>
              </tr>
              <tr>
                <td class="tg-zv4m"><span style="color: hsl(204, 86%, 53%);">ProX</span></td>
                <td class="tg-aw21">46.8</td>
                <td class="tg-aw21">48.4</td>
                <td class="tg-aw21">7 / 8</td>
              </tr>
            </tbody></table>
      </div>



      <br>

      <div class="column is-full-width">
        <h2 class="title is-4">Applying ProX on Domain-specific Continual Pre-training</h2>

        <p>We also demonstrate the potential of ProX on the continual pre-training scenario, specifically, in the mathematical domain. We apply the very same pipeline as in general domains to the already cleaned OpenWebMath corpus. </p>

        <br>

        <p><i>All models are tested using few-shot CoT prompts. Llemma and InternLM2-Math are continual pre-trained models from CodeLlama and InternLM2 with public available data, respectively. Note that the unique tokens and training tokens in the column refer exclusively to the token numbers from math-specific corpora (calculated by corresponding tokenizers). <sup>+</sup>: MQA evaluation of InternLM2 is based on an alternative prompt due to non-prediction issues with the original prompt. The <b>bold</b> entries represent the best results within the same base model. </i>
        </p>

        <br/>

        <style type="text/css">
            .tg  {border-collapse:collapse;border-spacing:0;}
            .tg td{border-color:#ffffff;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
              overflow:hidden;padding:10px 5px;word-break:normal;}
            .tg th{border-color:#ffffff;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
              font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
            .tg .tg-c3ow{border-color:#ffffff;text-align:center;vertical-align:top}
            .tg .tg-c3ow-hl{border-color:#ffffff;color: hsl(204, 86%, 53%);text-align:center;vertical-align:top}
            .tg .tg-fymr{border-color:#ffffff;font-weight:bold;text-align:left;vertical-align:top}
            .tg .tg-fymr-hl{border-color:#ffffff;color: hsl(204, 86%, 53%);text-align:left;vertical-align:top}
            .tg .tg-7btt{border-color:#ffffff;font-weight:bold;text-align:center;vertical-align:top}
            .tg .tg-7btt-hl{border-color:#ffffff;color: hsl(204, 86%, 53%);font-weight:bold;text-align:center;vertical-align:top}
            .tg .tg-0pky{border-color:#ffffff;text-align:left;vertical-align:top}
            .tg .tg-0pky-hl{border-color:#ffffff;color: hsl(204, 86%, 53%);text-align:left;vertical-align:top}
            </style>
            <table class="tg"><thead>
              <tr>
                <th class="tg-fymr">Model</th>
                <th class="tg-7btt">Size</th>
                <th class="tg-fymr">Method</th>
                <th class="tg-7btt">Uniq<br>Toks</th>
                <th class="tg-7btt">Train<br>Toks</th>
                <th class="tg-7btt">GSM8K</th>
                <th class="tg-7btt">MATH</th>
                <th class="tg-7btt">SVAMP</th>
                <th class="tg-7btt">ASDiv</th>
                <th class="tg-7btt">MAWPS</th>
                <th class="tg-7btt">TAB</th>
                <th class="tg-7btt">MQA</th>
                <th class="tg-7btt">MMLU STEM</th>
                <th class="tg-7btt">MATH</th>
                <th class="tg-7btt">AVG</th>
              </tr></thead>
            <tbody>
              <tr>
                <td class="tg-c3ow" colspan="15"><b>Existing Continual Pre-training for Reference</b></td>
              </tr>
              <tr>
                <td class="tg-0pky" rowspan="2">DeepSeek-LLM</td>
                <td class="tg-0pky">1.3B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">2.9</td>
                <td class="tg-c3ow">3.0</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">19.5</td>
                <td class="tg-c3ow">15.6</td>
                <td class="tg-c3ow">-</td>
              </tr>
              <tr>
                <td class="tg-0pky">1.3B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">14B</td>
                <td class="tg-c3ow">150B</td>
                <td class="tg-c3ow">11.5</td>
                <td class="tg-c3ow">8.9</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">29.6</td>
                <td class="tg-c3ow">31.3</td>
                <td class="tg-c3ow">-</td>
              </tr>
              <tr>
                <td class="tg-0pky" rowspan="2">CodeLlama (Base)</td>
                <td class="tg-0pky">7B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">11.8</td>
                <td class="tg-c3ow">5.0</td>
                <td class="tg-c3ow">44.2</td>
                <td class="tg-c3ow">50.7</td>
                <td class="tg-c3ow">62.6</td>
                <td class="tg-c3ow">30.6</td>
                <td class="tg-c3ow">14.3</td>
                <td class="tg-c3ow">20.4</td>
                <td class="tg-c3ow">21.9</td>
                <td class="tg-c3ow">29.1</td>
              </tr>
              <tr>
                <td class="tg-0pky">34B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">31.8</td>
                <td class="tg-c3ow">10.8</td>
                <td class="tg-c3ow">61.9</td>
                <td class="tg-c3ow">66.0</td>
                <td class="tg-c3ow">83.4</td>
                <td class="tg-c3ow">51.6</td>
                <td class="tg-c3ow">23.7</td>
                <td class="tg-c3ow">43.0</td>
                <td class="tg-c3ow">53.1</td>
                <td class="tg-c3ow">47.3</td>
              </tr>
              <tr>
                <td class="tg-0pky" rowspan="2">Llemma</td>
                <td class="tg-0pky">7B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">55B</td>
                <td class="tg-c3ow">200B</td>
                <td class="tg-c3ow">38.8</td>
                <td class="tg-c3ow">17.2</td>
                <td class="tg-c3ow">56.1</td>
                <td class="tg-c3ow">69.1</td>
                <td class="tg-c3ow">82.4</td>
                <td class="tg-c3ow">48.7</td>
                <td class="tg-c3ow">41.0</td>
                <td class="tg-c3ow">45.4</td>
                <td class="tg-c3ow">59.4</td>
                <td class="tg-c3ow">50.9 (+21.8)</td>
              </tr>
              <tr>
                <td class="tg-0pky">34B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">55B</td>
                <td class="tg-c3ow">50B</td>
                <td class="tg-c3ow">54.2</td>
                <td class="tg-c3ow">23.0</td>
                <td class="tg-c3ow">67.9</td>
                <td class="tg-c3ow">75.7</td>
                <td class="tg-c3ow">90.1</td>
                <td class="tg-c3ow">57.9</td>
                <td class="tg-c3ow">49.8</td>
                <td class="tg-c3ow">54.7</td>
                <td class="tg-c3ow">68.8</td>
                <td class="tg-c3ow">60.1 (+12.8)</td>
              </tr>
              <tr>
                <td class="tg-0pky" rowspan="2">InternLM2</td>
                <td class="tg-0pky">7B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">27.0</td>
                <td class="tg-c3ow">6.6</td>
                <td class="tg-c3ow">49.0</td>
                <td class="tg-c3ow">59.3</td>
                <td class="tg-c3ow">74.8</td>
                <td class="tg-c3ow">40.1</td>
                <td class="tg-c3ow">20.9 <sup>+</sup></td>
                <td class="tg-c3ow">19.0</td>
                <td class="tg-c3ow">28.1</td>
                <td class="tg-c3ow">36.1</td>
              </tr>
              <tr>
                <td class="tg-0pky">20B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">50.6</td>
                <td class="tg-c3ow">18.8</td>
                <td class="tg-c3ow">72.5</td>
                <td class="tg-c3ow">75.9</td>
                <td class="tg-c3ow">93.9</td>
                <td class="tg-c3ow">45.4</td>
                <td class="tg-c3ow">33.1</td>
                <td class="tg-c3ow">53.7</td>
                <td class="tg-c3ow">59.4</td>
                <td class="tg-c3ow">55.9</td>
              </tr>
              <tr>
                <td class="tg-0pky" rowspan="2">InternLM2-Math</td>
                <td class="tg-0pky">7B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">31B</td>
                <td class="tg-c3ow">125B</td>
                <td class="tg-c3ow">41.8</td>
                <td class="tg-c3ow">14.4</td>
                <td class="tg-c3ow">61.6</td>
                <td class="tg-c3ow">66.8</td>
                <td class="tg-c3ow">83.7</td>
                <td class="tg-c3ow">50.0</td>
                <td class="tg-c3ow">57.3</td>
                <td class="tg-c3ow">24.8</td>
                <td class="tg-c3ow">37.5</td>
                <td class="tg-c3ow">48.7 (+12.6)</td>
              </tr>
              <tr>
                <td class="tg-0pky">20B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">120B</td>
                <td class="tg-c3ow">500B</td>
                <td class="tg-c3ow">65.4</td>
                <td class="tg-c3ow">30.0</td>
                <td class="tg-c3ow">75.7</td>
                <td class="tg-c3ow">79.3</td>
                <td class="tg-c3ow">94.0</td>
                <td class="tg-c3ow">50.9</td>
                <td class="tg-c3ow">38.5</td>
                <td class="tg-c3ow">53.1</td>
                <td class="tg-c3ow">71.9</td>
                <td class="tg-c3ow">62.1 (+6.2)</td>
              </tr>
              <tr>
                <td class="tg-c3ow" colspan="15"><b>Applying Data Refinement Approaches</b></td>
              </tr>
              <tr>
                <td class="tg-0pky">TinyLlama (Base)</td>
                <td class="tg-0pky">1.1B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">2.8</td>
                <td class="tg-c3ow">3.2</td>
                <td class="tg-c3ow">10.9</td>
                <td class="tg-c3ow">18.0</td>
                <td class="tg-c3ow">20.2</td>
                <td class="tg-c3ow">12.5</td>
                <td class="tg-c3ow">14.6</td>
                <td class="tg-c3ow">16.4</td>
                <td class="tg-c3ow">21.9</td>
                <td class="tg-c3ow">14.7</td>
              </tr>
              <tr>
                <td class="tg-0pky" rowspan="4"><span style="font-weight:400;font-style:normal">TinyLlama (CPT)</span></td>
                <td class="tg-0pky">1.1B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">15B</td>
                <td class="tg-c3ow">15B</td>
                <td class="tg-c3ow">6.2</td>
                <td class="tg-c3ow">4.8</td>
                <td class="tg-c3ow">22.3</td>
                <td class="tg-c3ow">36.2</td>
                <td class="tg-c3ow">47.6</td>
                <td class="tg-c3ow">19.3</td>
                <td class="tg-c3ow">11.6</td>
                <td class="tg-c3ow">20.7</td>
                <td class="tg-c3ow">25.0</td>
                <td class="tg-c3ow">21.5 (+8.1)</td>
              </tr>
              <tr>
                <td class="tg-0pky">1.1B</td>
                <td class="tg-0pky">Rho</td>
                <td class="tg-c3ow">15B</td>
                <td class="tg-c3ow">9B</td>
                <td class="tg-c3ow">7.1</td>
                <td class="tg-c3ow">5.0</td>
                <td class="tg-c3ow">23.5</td>
                <td class="tg-c3ow">41.2</td>
                <td class="tg-c3ow">53.8</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-7btt">18.0</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
              </tr>
              <tr>
                <td class="tg-0pky">1.1B</td>
                <td class="tg-0pky">Rule</td>
                <td class="tg-c3ow">6.5B</td>
                <td class="tg-c3ow">15B</td>
                <td class="tg-c3ow">4.5</td>
                <td class="tg-c3ow">2.8</td>
                <td class="tg-c3ow">17.5</td>
                <td class="tg-c3ow">29.4</td>
                <td class="tg-c3ow">39.3</td>
                <td class="tg-c3ow">15.1</td>
                <td class="tg-c3ow">12.4</td>
                <td class="tg-c3ow">19.4</td>
                <td class="tg-c3ow">25.0</td>
                <td class="tg-c3ow">18.4 (+3.7)</td>
              </tr>
              <tr>
                <td class="tg-0pky-hl">1.1B</td>
                <td class="tg-0pky-hl">ProX</td>
                <td class="tg-c3ow-hl">5B</td>
                <td class="tg-c3ow-hl">15B</td>
                <td class="tg-7btt-hl">9.0</td>
                <td class="tg-7btt-hl">5.6</td>
                <td class="tg-7btt-hl">23.8</td>
                <td class="tg-7btt-hl">41.9</td>
                <td class="tg-7btt-hl">56.9</td>
                <td class="tg-7btt-hl">22.2</td>
                <td class="tg-c3ow-hl">15.6</td>
                <td class="tg-7btt-hl">26.8</td>
                <td class="tg-7bttv-hl">31.2</td>
                <td class="tg-7btt-hl">25.7 (+11.0)</td>
              </tr>
              <tr>
                <td class="tg-0pky">Llama-2 (Base)</td>
                <td class="tg-0pky">7B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">14.1</td>
                <td class="tg-c3ow">3.8</td>
                <td class="tg-c3ow">39.5</td>
                <td class="tg-c3ow">51.6</td>
                <td class="tg-c3ow">63.6</td>
                <td class="tg-c3ow">30.9</td>
                <td class="tg-c3ow">12.5</td>
                <td class="tg-c3ow">32.9</td>
                <td class="tg-c3ow">34.4</td>
                <td class="tg-c3ow">31.5</td>
              </tr>
              <tr>
                <td class="tg-0pky" rowspan="2">Llama-2 (CPT)</td>
                <td class="tg-0pky">7B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">15B</td>
                <td class="tg-c3ow">10B</td>
                <td class="tg-c3ow">29.6</td>
                <td class="tg-c3ow">13.6</td>
                <td class="tg-c3ow">49.2</td>
                <td class="tg-c3ow">61.9</td>
                <td class="tg-c3ow">78.4</td>
                <td class="tg-c3ow">36.3</td>
                <td class="tg-c3ow">31.9</td>
                <td class="tg-c3ow">40.5</td>
                <td class="tg-c3ow">43.8</td>
                <td class="tg-c3ow">42.8 (+11.3)</td>
              </tr>
              <tr>
                <td class="tg-0pky-hl">7B</td>
                <td class="tg-0pky-hl">ProX</td>
                <td class="tg-c3ow-hl">5B</td>
                <td class="tg-c3ow-hl">10B</td>
                <td class="tg-7btt-hl">30.6</td>
                <td class="tg-7btt-hl">16.8</td>
                <td class="tg-7btt-hl">50.2</td>
                <td class="tg-7btt-hl">63.7</td>
                <td class="tg-7btt-hl">79.3</td>
                <td class="tg-7btt-hl">37.3</td>
                <td class="tg-7btt-hl">40.1</td>
                <td class="tg-7btt-hl">43.8</td>
                <td class="tg-7btt-hl">53.1</td>
                <td class="tg-7btt-hl">46.1 (+14.6)</td>
              </tr>
              <tr>
                <td class="tg-0pky">CodeLlama (Base)</td>
                <td class="tg-0pky">7B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">11.8</td>
                <td class="tg-c3ow">5.0</td>
                <td class="tg-c3ow">44.2</td>
                <td class="tg-c3ow">50.7</td>
                <td class="tg-c3ow">62.6</td>
                <td class="tg-c3ow">30.6</td>
                <td class="tg-c3ow">14.3</td>
                <td class="tg-c3ow">20.4</td>
                <td class="tg-c3ow">21.9</td>
                <td class="tg-c3ow">29.1</td>
              </tr>
              <tr>
                <td class="tg-0pky" rowspan="2">CodeLlama (CPT)</td>
                <td class="tg-0pky">7B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">15B</td>
                <td class="tg-c3ow">10B</td>
                <td class="tg-c3ow">31.1</td>
                <td class="tg-c3ow">14.8</td>
                <td class="tg-c3ow">51.4</td>
                <td class="tg-c3ow">62.1</td>
                <td class="tg-c3ow">81.2</td>
                <td class="tg-c3ow">33.6</td>
                <td class="tg-c3ow">30.4</td>
                <td class="tg-c3ow">40.5</td>
                <td class="tg-c3ow">43.8</td>
                <td class="tg-c3ow">43.2 (+14.1)</td>
              </tr>
              <tr>
                <td class="tg-0pky-hl">7B</td>
                <td class="tg-0pky-hl">ProX</td>
                <td class="tg-c3ow-hl">5B</td>
                <td class="tg-c3ow-hl">10B</td>
                <td class="tg-7btt-hl">35.6</td>
                <td class="tg-7btt-hl">17.6</td>
                <td class="tg-7btt-hl">55.8</td>
                <td class="tg-7btt-hl">67.9</td>
                <td class="tg-7btt-hl">82.7</td>
                <td class="tg-7btt-hl">41.3</td>
                <td class="tg-7btt-hl">38.9</td>
                <td class="tg-7btt-hl">42.6</td>
                <td class="tg-7btt-hl">62.5</td>
                <td class="tg-7btt-hl">49.4 (+20.3)</td>
              </tr>
              <tr>
                <td class="tg-0pky">Mistral (Base)</td>
                <td class="tg-0pky">7B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">-</td>
                <td class="tg-c3ow">40.6</td>
                <td class="tg-c3ow">11.4</td>
                <td class="tg-7btt">65.4</td>
                <td class="tg-c3ow">68.5</td>
                <td class="tg-c3ow">87.0</td>
                <td class="tg-7btt">52.9</td>
                <td class="tg-c3ow">32.3</td>
                <td class="tg-c3ow">50.0</td>
                <td class="tg-c3ow">56.2</td>
                <td class="tg-c3ow">51.6</td>
              </tr>
              <tr>
                <td class="tg-0pky" rowspan="2">Mistral (CPT)</td>
                <td class="tg-0pky">7B</td>
                <td class="tg-0pky">-</td>
                <td class="tg-c3ow">15B</td>
                <td class="tg-c3ow">10B</td>
                <td class="tg-c3ow">44.4</td>
                <td class="tg-c3ow">19.2</td>
                <td class="tg-c3ow">65.2</td>
                <td class="tg-c3ow">69.6</td>
                <td class="tg-c3ow">88.4</td>
                <td class="tg-c3ow">46.6</td>
                <td class="tg-c3ow">43.1</td>
                <td class="tg-c3ow">50.8</td>
                <td class="tg-c3ow">65.6</td>
                <td class="tg-c3ow">54.8 (+3.2)</td>
              </tr>
              <tr>
                <td class="tg-0pky-hl">7B</td>
                <td class="tg-0pky-hl">ProX</td>
                <td class="tg-c3ow-hl">4.7B</td>
                <td class="tg-c3ow-hl">10B</td>
                <td class="tg-c3ow-hl">51.0</td>
                <td class="tg-7btt-hl">22.4</td>
                <td class="tg-c3ow-hl">64.9</td>
                <td class="tg-7btt-hl">72.9</td>
                <td class="tg-7btt-hl">89.2</td>
                <td class="tg-c3ow-hl">49.8</td>
                <td class="tg-7btt-hl">53.0</td>
                <td class="tg-7btt-hl">54.2</td>
                <td class="tg-7btt-hl">75.0</td>
                <td class="tg-7btt-hl">59.2 (+7.6)</td>
              </tr>
            </tbody></table>

    <br> 
    <p><i>Please refer to our paper for more details.</i></p>
      </div>
    
    <br>

    <!-- add te banner image in the center -->
      <div class="column is-full-width">
        <h2 class="title is-3">What's More?</h2>
        
        <p>ProX might just be the <i>art moment</i> for data engineering in language model pre-training. It's a pioneering effort to optimize pre-trained corpora through various advanced operations powered by language models at scale. </p>
        <br>
        <p><b>But this is only the beginning! Stay tuned for more exciting updates, and follow us on <a href="https://huggingface.co/gair-prox">Hugging Face</a> and <a href="https://github.com/GAIR-NLP/ProX">GitHub</a> for the latest developments. </b></p>

        <div class="content has-text-justified">
          <p>
            <img src="./static/images/prox-logo.png" width="120%" style="padding-left: 0%;padding-right:0%;">
          </p>
        </div>
      </div>

    </div>


  </div>



  </div>
</section>


</div>
</section>



<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{2024prox,
  title={Programming Every Example: Lifting Pre-training Data Quality Like Experts at Scale},
  author={},
  journal={arXiv preprint arXiv:xxxx.xxxxx},
  year={2024}
}</code></pre>
    </div>
</section>

<footer class="footer">
    <div class="columns is-centered">
        <div class="column is-8">
            <div class="content">
                <p>
                    This website is licensed under a <a rel="license"
                                                        href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                    Commons Attribution-ShareAlike 4.0 International License</a>.
                </p>
                <p>
                    This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                    code</a> of this website,
                    we just ask that you link back to this page in the footer.
                    Please remember to remove the analytics code included in the header of the website which
                    you do not want on your website.
                </p>
            </div>
        </div>
    </div>
    </div>
</footer>

</body>
</html>
