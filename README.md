# Programming Every Example: Lifting Pre-training Data Quality Like Experts at Scale
<p align="center">
  <img src="./static/images/prox-logo.png">
</p>
<a href="https://huggingface.co/gair-prox" target="_blank">
    <img alt="Models" src="https://img.shields.io/badge/ðŸ¤—-HuggingFace Repo-blue" />
</a>
<a href="https://arxiv.org/abs/xxxx.xxxxx" target="_blank">
    <img alt="Paper" src="https://img.shields.io/badge/ðŸ“‘-Paper-blue" />
</a>
<a href="https://gair-nlp.github.io/program-every-example/" target="_blank">
<img alt="Project Page" src="https://img.shields.io/badge/ðŸ§ª-Project Page-blue" />
</a>
<a href="https://opensource.org/license/apache-2-0" target="_blank">
    <img alt="License: apache-2-0" src="https://img.shields.io/github/license/saltstack/salt" />
</a>
<a href="https://github.com/GAIR-NLP/program-every-example" target="_blank">
    <img alt="GitHub Stars" src="https://img.shields.io/github/stars/GAIR-NLP/program-every-example?style=social" />
</a>
<a href="https://github.com/GAIR-NLP/program-every-example/issues" target="_blank">
    <img alt="Open Issues" src="https://img.shields.io/github/issues-raw/GAIR-NLP/program-every-example" />
</a>

## ðŸ”¥ News
* **[19 September, 2024]:** ðŸŽ‰ We open-sourced [pre-training corpus](https://huggingface.co/collections/gair-prox/prox-dataset-66e81c9d560911b836bb3704) curated by our ProX framework, containing > 100B high quality general domain corpus and ~5B high quality math corpus, together with [models](https://huggingface.co/collections/gair-prox/prox-math-models-66e92c3e5d54b27612286eb9) trained using these data.

## ðŸš€ Introduction